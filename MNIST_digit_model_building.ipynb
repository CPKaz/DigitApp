{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_digit_model_building.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BtbCjkOCt74l",
        "a-QH0Xynpj7M",
        "GqrhgmWzCQq_",
        "MZ2Z5mxYpaWI",
        "Ase7JrEiuszq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtbCjkOCt74l"
      },
      "source": [
        "# MNIST model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ytt7JvWn4RM"
      },
      "source": [
        "Summary:\n",
        "This notebook was used to generate the model used in the app found here: \n",
        "https://cpkaz.github.io/Home/DigitApp.html\n",
        "I also include commentary for anyone interested in the making of the app.\n",
        "\n",
        "I specifically picked this data, because I wanted to deliver an ML experience that anyone could underestand. I wanted users to be able to be able to interact with my project in a fun way by drawing their input. The MNIST data is black and white, and easy to replicate with users drawing with a mouse on a simple html canvas. Thus, the MNIST data was a clear winner, given the content I wanted to produce.\n",
        "\n",
        "  This data comes with one drawback, in that it's so well-documented that it's hard to do anything remarkable with it in terms of modelling. There are thousands of pre-trained models out there for these specific images. I could have imported one, but I elected to train mine from scractch. I generated new data by shifting and rotating some of the images at random, and then used a simple convolutional framework for the digit recognition.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjgpsLse5FY5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784')\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POSYBxfchW2x"
      },
      "source": [
        "X = np.reshape(X, [70000, 28, 28, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-QH0Xynpj7M"
      },
      "source": [
        "# Applying Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFban75vuCI9"
      },
      "source": [
        "Below are a few functions I wrote to apply transformations to the data. I anticipate that the users of my web page might write in ways that will confuse the model, so I'm modifying the existing images to train the model to recognize the numbers when they're slightly out of place. Here, I add 90 thousand additional images, with shifts of 3 pixels upward, downward, left, right, and 15 degree rotations in either direction.\n",
        "\n",
        "I got the ideas for these transformations from this article:\n",
        "https://towardsdatascience.com/going-beyond-99-mnist-handwritten-digits-recognition-cfff96337392"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joOnnKGCH5UQ"
      },
      "source": [
        "def shift_right(arr, n):\n",
        "  if n > 0:\n",
        "    arr = np.roll(arr, n, axis=1)\n",
        "    arr[:, 0:n] = 0\n",
        "  else:\n",
        "    l = len(arr[0])+1\n",
        "    arr = np.roll(arr, n, axis=1)\n",
        "    f = l + n - 1\n",
        "    arr[:, f:l] = 0\n",
        "  return arr\n",
        "\n",
        "def shift_up(arr, n):\n",
        "  if n > 0:\n",
        "    arr = np.roll(arr, -1*n, axis=0)\n",
        "    l = len(arr[0])\n",
        "    f = l - n - 1\n",
        "    arr[f:l, :] = 0\n",
        "  else:\n",
        "    arr = np.roll(arr, -1*n, axis=0)\n",
        "    arr[0:-1*n, :] = 0\n",
        "  return arr\n",
        "\n",
        "def rot(img, deg):\n",
        "  i = ndimage.rotate(img, deg, reshape=False)\n",
        "  for k in range(28):\n",
        "    for j in range(28):\n",
        "      if i[k][j] > 255:\n",
        "        i[k][j] = 255\n",
        "      elif i[k][j] < 50:\n",
        "        i[k][j] = 0\n",
        "  return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_JufCHDRrUy"
      },
      "source": [
        "rR = np.random.choice(range(70000), 15000, replace=False)\n",
        "rL = np.random.choice(range(70000), 15000, replace=False)\n",
        "sU = np.random.choice(range(70000), 15000, replace=False)\n",
        "sD = np.random.choice(range(70000), 15000, replace=False)\n",
        "sR = np.random.choice(range(70000), 15000, replace=False)\n",
        "sL = np.random.choice(range(70000), 15000, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjX3YPlKcWGD"
      },
      "source": [
        "numbers = [rR, rL, sU, sD, sR, sL]\n",
        "functions = [rot, rot, shift_up, shift_up, shift_right, shift_right]\n",
        "values = [15, -15, 3, -3, 3, -3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mZ_RCUbgIK_"
      },
      "source": [
        "new_ys = []\n",
        "for j in numbers:\n",
        "  for k in j:\n",
        "    new_ys.append(y[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTsoDE6XADiW"
      },
      "source": [
        "transform_array = []\n",
        "for nums, func, val  in zip(numbers, functions, values):\n",
        "  for i in nums:\n",
        "    t = func(X[i], val)\n",
        "    transform_array.append(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBK2EqJ3hr_3"
      },
      "source": [
        "tar = np.array(transform_array)\n",
        "X = np.concatenate((X, tar))\n",
        "y = np.concatenate((y, new_ys))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqrhgmWzCQq_"
      },
      "source": [
        "# Example Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaiNKzM7Z6-g"
      },
      "source": [
        "Below is an image drawn from the data, along with a rotated version. I originally rotated the numbers by 30 degrees, but the angle was too severe so many of the numbers were at a much steeper angle than what a human might draw. 15 degrees was a good balance between creating observations that differed from the original data, and generating images that a human might actually draw."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "yOf_-ajNfTe6",
        "outputId": "7831f367-83b8-4ec9-aa96-1fc1e75df2de"
      },
      "source": [
        "plt.subplot(131)\n",
        "plt.imshow(np.reshape(X[rL[4]], (28, 28)), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f50d2259a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "kBquM5TPVilK",
        "outputId": "9fc0159c-bde1-45ab-8309-6f79beee174d"
      },
      "source": [
        "plt.subplot(131)\n",
        "plt.imshow(np.reshape(transform_array[10004], (28, 28)), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f50d22354d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfeOgnJHbMAe"
      },
      "source": [
        "Below is an image drawn with the html canvas, it's much brighter, as the html pen in only one color, white. Any dark areas are caused by compression to 28x28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "_wfT3-lgsJGN",
        "outputId": "3f1431d7-e8b8-46a2-97f3-e91c43580970"
      },
      "source": [
        "digit = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,253,173,96,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,178,28,0,0,0,0,0,0,0,0,0,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,17,88,238,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,103,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,222,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,170,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,62,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,178,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,34,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,42,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "digit_pixels = digit.reshape(28, 28)\n",
        "plt.subplot(131)\n",
        "plt.imshow(digit_pixels, cmap='gray')\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAYAAADG4PRLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACaElEQVR4nO3dPY7iMABA4WQ1HICSlpZDUHIUJA6EKKmpENyAeyAaCu6ARLaLkhHyJBBm/Oz3VWGZn0hPttdJli2rqirE9e+vT0DvMSCcAeEMCGdAuK/Qm2VZ+lfUSFRVVT77c0cgnAHhDAhnQDgDwhkQzoBwBoQzIJwB4QwIZ0A4A8IF70Z8f17m8XjUx/v9vvXe8Xisj7fb7QCnpi4cgXAGhDMgXPnDc6FZ3ZFvruNFURSr1ao+vl6vv306Ld6RT5QB4ZxCP6wsn858vTmFJsqAcAaE++nB3tbr8XhcHy8Wi9Z76/X66dflbjQatV7f7/dBf74jEM6AcMFtRG7/NmK5XLZebzabt3+m2wgFGRDOgHCugR31+TSPy+VSH0+n06F+v2tgigwI5xQa8OqHIA21dWhyCk2UAeEMCBe8G5GbmNa8rhyBcAaEy34KJU6bTY5AOAPCGRAuuzWQvuZ95wiEMyBc8lPoO/+tQqzTZpMjEM6AcAaES3INTG2rEOIIhDMgnAHhklgDd7vdy99LXPeaHIFwBoRL4sHePtuG+Xzeen06nYY+nY/wwd5EGRDOgHDYNTCny2VF4RqYLAPCYa7EHA6Hl76POmV25QiEMyCcAeEw24g+24bJZFIf3263T5zOr3MbkSgDwkU9heZ2tSXEKTRRBoQzIBzmUlpIimteV45AOAPCRTWFDvXhqDlxBMIZEM6AcFGtgefzufPXzmazD54JhyMQzoBwUd2N6HP3IberL96NSJQB4QwIZ0A4A8IZEC6qKzG5bQ2G4AiEMyCcAeGCl9IUP0cgnAHhDAhnQDgDwhkQ7j9EBaBrV92RkQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Ej8BFMkqk42j",
        "outputId": "fe75c5d9-2642-4146-b488-cf471a3effa3"
      },
      "source": [
        "dp = shift_up(digit_pixels, -4)\n",
        "plt.subplot(131)\n",
        "plt.imshow(dp, cmap='gray')\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAYAAADG4PRLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACSUlEQVR4nO3dsW3CQABAUV8UBqCkZRFKRkFiIERJTYVgAxahoWAHJJzOshE6bDDE/+6/yoSEWPq6u9jBJpRlWYjr5793QO8xIJwB4QwIZ0C439iTIQT/RB2IsizDo687AuEMCGdAOAPCGRDOgHAGhDMgnAHhDAhnQDgDwhkQzoBwBoQzIJwB4QwIZ0A4A8IZEM6AcAaEMyCcAeEMCGdAuOi1EfdX795ut2p7t9s1njscDtX2ZrPpYdfUhiMQzoBwBoQLT+5SkdX1gfV1vCiKYrlcVtvn8/nbu9Pg9YGJMiCcU+iHhfBw5uvMKTRRBoQzINyz24w0Ho/H42p7Pp83nlutVg+/L3ej0ajx+Hq99vr6jkA4A8JFDyNyu1PTYrFoPF6v12+/pocRijIgnAHhXANb6nJv8dPpVG1Pp9O+fr9rYIoMCOcUGvHqRzL0dehQ5xSaKAPCGRAu+t+I3AxpzWvLEQhnQDgDwmW/BhLXvTpHIJwB4bKbQulT5j1HIJwB4QwIl/wa+M6ndA913atzBMIZEC7JKTS1Q4UYRyCcAeEMCJfEGrjdbl/+WeK6V+cIhDMgXBJv7O1y2DCbzRqPj8dj37vzEb6xN1EGhDMgHHYNzOl0WVG4BibLgHCYMzH7/f6ln6NOmW05AuEMCGdAOMxhRJfDhslkUm1fLpdP7M7XeRiRKAPCDXoKze1sS4xTaKIMCGdAOMyptJgU17y2HIFwBoQb1BTa181Rc+IIhDMgnAHhnn1+oAbOEQhnQDgDwhkQzoBwBoT7A/gek1CAwIClAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ2Z5mxYpaWI"
      },
      "source": [
        "# Setting up the Data and Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYRhPiqH2an"
      },
      "source": [
        "digits = []\n",
        "for i in range(len(X)):\n",
        "  digits.append([X[i], y[i]])\n",
        "train, test = train_test_split(digits, test_size=.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJYkjT_vJ3Hi"
      },
      "source": [
        "X_train = np.array([a[0] for a in train])\n",
        "y_train = np.array([np.float64(a[1]) for a in train])\n",
        "\n",
        "X_test = np.array([a[0] for a in test])\n",
        "y_test = np.array([np.float64(a[1]) for a in test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NapucID4BXF8"
      },
      "source": [
        "Initially, I just used a simple dense neural network, which did reasonably well. However, the model didn't do well in the field, so I switch to a CNN. I tried a few different configurations. Many of them had more filters and layers, but this simple one did the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dnSl-2mJTUt"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(.4))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mob8KQ4ACjrY"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RA_9DyvnUE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6da254-c390-4c1c-b8c9-c979484318f7"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6400/6400 [==============================] - 74s 11ms/step - loss: 1.2223\n",
            "Epoch 2/5\n",
            "6400/6400 [==============================] - 74s 12ms/step - loss: 0.2846\n",
            "Epoch 3/5\n",
            "6400/6400 [==============================] - 76s 12ms/step - loss: 0.2709\n",
            "Epoch 4/5\n",
            "6400/6400 [==============================] - 73s 11ms/step - loss: 0.2615\n",
            "Epoch 5/5\n",
            "6400/6400 [==============================] - 73s 11ms/step - loss: 0.2594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f50d2854a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4n9qTZEAb8"
      },
      "source": [
        "y_pred = [np.argmax(i) for i in model.predict(X_test)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EngRTN1Gu6Qn"
      },
      "source": [
        "The accuracy isn't the highest I've seen compared to other models using MNIST digit data, but I'm not concerned. The final product will have to take in data that doesn't look exactly like what it sees in this dataset. The javascript canvas drawings can't mimic the writing seen in these images exactly, so overfitting could greatly diminish performance. Tuning the model to get higher test scores might not necessarily make it perform better in the field, and could even make it worse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI0vCB1HPOG7",
        "outputId": "61f70d77-1872-4c7a-a424-65c3c05ec1ff"
      },
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95434375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWRm9qaCeVi"
      },
      "source": [
        "# Exporting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNc8v-qXxr5H"
      },
      "source": [
        "import tensorflowjs as tfjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR7evXe0x5hz"
      },
      "source": [
        "tfjs.converters.save_keras_model(model, 'model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LErmXJfA-Ja"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ase7JrEiuszq"
      },
      "source": [
        "# Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBuDfB6zu3K2"
      },
      "source": [
        "I'm happy with the results of this project. The digit identifier does pretty well, even though the input data differs from the sample data. The two biggest differences are:\n",
        " \n",
        " 1. The color on the html canvas is plain white, while the training data includes white in addition to various shades of grey.\n",
        "  \n",
        "  2. The shape of the markings made by the mouse aren't shaped exactly like the markings of the writing utensils used to make the sample data.\n",
        "\n",
        "I think that the results could be improved by adding more transformations to the sample, and removing certain images that resemble the computer drawings the least. The first method would expose the model to a wider variety of representations of numbers, and allow it to identify more obscure drawings more effectively. The second method would remove observations that aren't reflective of the population we're interested in modeling (the mouse-drawn digits).\n",
        "\n",
        "Lastly, the biggest problem with this project is that I don't have a scalable way to verify that the model is working. I've tested by myself with dozens of drawings, but I don't have an effective way to measure how well the model is doing. I could save the images users submit, along with a user-provided label of the image and use those for further training, but I'm not sure if there's an effective way to do that with GitHub Pages. Another issue with that approach is that someone could feed the model intentionally wrong data. I could also use a labeling service to generate more data exclusively using my html canvas, but that would cost money."
      ]
    }
  ]
}